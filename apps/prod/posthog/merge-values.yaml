apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: posthog
  namespace: posthog
spec:
  # https://github.com/compose-ai/charts-clickhouse/blob/main/charts/posthog/values.yaml
  values:
    # -- Site url specifies the canonical URL root the site can be accessed using.
    # This is used to e.g. generate shareable links to Dashboards.
    siteUrl: "https://posthog.azure-prod.cluster.compose.ai"

    web:
      env:
        # -- Set google oauth 2 key. Requires posthog ee license.
        - name: SOCIAL_AUTH_GOOGLE_OAUTH2_KEY
          value:
        # -- Set google oauth 2 secret. Requires posthog ee license.
        - name: SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET
          value:
        # -- Set google oauth 2 whitelisted domains users can log in from.
        - name: SOCIAL_AUTH_GOOGLE_OAUTH2_WHITELISTED_DOMAINS
          value: "compose.ai"
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    worker:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    plugins:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    pluginsAsync:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    pluginsIngestion:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    pluginsAnalyticsIngestion:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    recordingsIngestion:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    recordingsBlobIngestion:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    pluginsExports:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    pluginsJobs:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    pluginsScheduler:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    temporalPyWorker:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    postgresql:
      master:
        nodeSelector:
          cloud.google.com/gke-nodepool: posthog
        tolerations:
          - key: "posthog"
            operator: "Exists"
            effect: "NoSchedule"
      persistence:
        # -- Enable persistence using PVC.
        enabled: true
        # -- PVC Storage Request for PostgreSQL volume.
        size: 100Gi
    _pgbouncer:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    redis:
      master:
        nodeSelector:
          cloud.google.com/gke-nodepool: posthog
        tolerations:
          - key: "posthog"
            operator: "Exists"
            effect: "NoSchedule"
    kafka:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      persistence:
        # - Enable data persistence using PVC.
        enabled: true
        # -- PVC Storage Request for Kafka data volume.
        size: 200Gi
      logRetentionBytes: _150_000_000_000
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    zookeeper:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    clickhouse:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
      layout:
        shardsCount: 5
        replicasCount: 1
      persistence:
        # -- Enable data persistence using PVC.
        enabled: true

        storageClassName: managed-csi-premium
        # -- Persistent Volume size
        size: 700Gi
      # We probably could update the backup script and do incremental backups by using the new scripts from here with the newest image too
      # https://github.com/Altinity/clickhouse-backup/tree/master
      backup:
        enabled: true
        azureSecret: posthog-blob-storage-secret
        clickhouse_services: "chi-posthog-posthog-0-0,chi-posthog-posthog-1-0,chi-posthog-posthog-2-0,chi-posthog-posthog-3-0,chi-posthog-posthog-4-0"
        env:
          - name: "REMOTE_STORAGE"
            value: "azblob"
          - name: ALLOW_EMPTY_BACKUPS
            value: "true"
          - name: API_LISTEN
            value: "0.0.0.0:7171"
          # INSERT INTO system.backup_actions to execute backup
          - name: API_CREATE_INTEGRATION_TABLES
            value: "true"
          - name: BACKUPS_TO_KEEP_REMOTE
            value: "0"
          - name: AZBLOB_USE_MANAGED_IDENTITY
            value: "true"
          - name: AZBLOB_ACCOUNT_NAME
            value: "composeclusterprod"
          - name: "AZBLOB_CONTAINER"
            value: "composeai-clickhouse-backups"
          - name: "AZBLOB_PATH"
            value: "backup/{cluster}/{shard}"
          - name: "AZBLOB_OBJECT_DISK_PATH"
            value: "object_disks/{cluster}/{shard}"
          - name: "AZBLOB_DEBUG"
            value: "true"
    hooks:
      nodeSelector:
        cloud.google.com/gke-nodepool: posthog
      tolerations:
        - key: "posthog"
          operator: "Exists"
          effect: "NoSchedule"
    ingress:
      # -- Ingress handler type. Defaults to `nginx` if nginx is enabled and to `clb` on gcp.
      # We're using our own so this doesn't actually do anything but we then put the ingress class annotation on the ingress which makes it get picked up by our posthog-specific nginx controller
      type: "posthog-nginx"
      annotations:
        kubernetes.io/ingress.class: "posthog-nginx"
      hostname: "posthog.prod.cluster.compose.ai"
