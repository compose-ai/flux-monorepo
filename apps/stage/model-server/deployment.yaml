---
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: model-server
  name: model-server
  namespace: model-server
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: model-server
  strategy:
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: model-server
    spec:
      nodeSelector:
        cloud.google.com/gke-nodepool: compose
      containers:
        - name: model-server
          image: us-docker.pkg.dev/compose-cluster/ea-bignlp/inference:22.08-py3
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash"]
          args: ["-c", "tritonserver --model-repository /models/nemo-megatron-gpt-1.3B/output --response-cache-byte-size 1073741824"]
          resources:
            requests:
              cpu: 2000m
              memory: 12G
            limits:
              nvidia.com/gpu: 1
          ports:
            - containerPort: 8000
              name: http
            - containerPort: 8001
              name: grpc
            - containerPort: 8002
              name: metrics
          livenessProbe:
            httpGet:
              path: /v2/health/live
              port: http
          readinessProbe:
            initialDelaySeconds: 15
            periodSeconds: 5
            httpGet:
              path: /v2/health/ready
              port: http
          volumeMounts:
            - name: models
              mountPath: /models
              readOnly: false
          securityContext:
            runAsUser: 1001
            runAsGroup: 1001
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: model-pv-claim
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: prepare-model
          image: us-docker.pkg.dev/compose-cluster/ea-bignlp/training:23.01-py3
          command: 
          - sh
          - "-c"
          - |
            /bin/bash -x <<'EOF'

            set -x
            export PYTHONPATH=/opt/bignlp/FasterTransformer:${PYTHONPATH}
            cd /opt/bignlp
            python3 FasterTransformer/examples/pytorch/gpt/utils/nemo_ckpt_convert.py \
                --in-file /models/nemo-megatron-gpt-1.3B/nemo_gpt1.3B_fp16.nemo \
                --infer-gpu-num 1 \
                --saved-dir /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b \
                --weight-data-type fp16 \
                --load-checkpoints-to-cpu 0
            python3 /opt/bignlp/bignlp-scripts/bignlp/collections/export_scripts/prepare_triton_model_config.py \
                --model-train-name gpt3_1.3b \
                --template-path /opt/bignlp/fastertransformer_backend/all_models/gpt/fastertransformer/config.pbtxt \
                --ft-checkpoint /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/1-gpu \
                --config-path /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/config.pbtxt \
                --max-batch-size 256 \
                --pipeline-model-parallel-size 1 \
                --tensor-model-parallel-size 1 \
                --data-type bf16
            cat <<EOT >> /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/header.pbtxt
            name: "gpt3_1.3b"
            max_batch_size: 256
            batch_input [
              {
                kind: BATCH_ITEM_SHAPE
                target_name: "input_ids_item_shape"
                data_type: TYPE_INT32
                source_input: "input_ids"
              }
            ]
            dynamic_batching {
              preferred_batch_size: [1,2,4,8,16,32,64]
              max_queue_delay_microseconds: 5000000
            }
            input {
              name: "input_ids"
              data_type: TYPE_UINT32
              dims: -1
              allow_ragged_batch: true
            }
            EOT
            sed -i '1,7d' /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/config.pbtxt
            head -n20 /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/header.pbtxt > \
                /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/complete.pbtxt
            cat /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/config.pbtxt >> /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/complete.pbtxt
            mv -i /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/complete.pbtxt /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/config.pbtxt --force
            rm -rf /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/header.pbtxt
            head -20 /models/nemo-megatron-gpt-1.3B/output/gpt3_1.3b/config.pbtxt
            set +x

            EOF
          volumeMounts:
            - name: models
              mountPath: /models
              readOnly: false
          securityContext:
            runAsUser: 1001
            runAsGroup: 1001
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: model-csi-gcs-sc
provisioner: gcs.csi.ofek.dev
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-pv-claim
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: model-csi-gcs-sc
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: model-csi-gcs-pv
  annotations:
    pv.beta.kubernetes.io/gid: "1001"
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 10Gi
  persistentVolumeReclaimPolicy: Retain
  storageClassName: model-csi-gcs-sc
  csi:
    driver: gcs.csi.ofek.dev
    volumeHandle: csi-gcs
    nodePublishSecretRef:
      name: csi-gcs-secret
      namespace: model-server
    volumeAttributes:
      gid: "1001"
      uid: "1001"
      dirMode: "0775"
      fileMode: "0664"
      implicitDirs: "true"
---
